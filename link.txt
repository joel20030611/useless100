PRACTICAL NO : 3

 

ii.)Assume there is a list of numbers where each number represents how many friends a person has. Use random number generator to create the above data. Find the mode of the data.

CODE :

import random

num_friends= [random.randint(1,100) for _ in range(100)]

from collections import Counter

def mode(x):

counts = Counter(x)

print(counts)

max_count = max(counts.values())

return [x_i for x_i, count in counts.items() if count == max_count]

print(mode(num_friends))

 

iii.) Dispersion - Assume there is a list of numbers where each number represents how many friends a person has. Use random number generator to create the above data. Find range, variance, standard deviation, interquartile range.

RANGE :

def data_range(x):

     return max(x) - min(x)

print(data_range(num_friends))

VARIANCE / STANDARD DEVIATION :

import math

def dot(v, w):

              return sum(v_i * w_i for v_i, w_i in zip(v, w))

def sum_of_squares(v):

    return dot(v, v)

def mean(x):

    return sum(x) / len(x)

def de_mean(x):

    x_bar = mean(x)

    return [x_i - x_bar for x_i in x]

def variance(x):

    n = len(x)

    deviations = de_mean(x)

    return sum_of_squares(deviations) / (n - 1)

def standard_deviation(x):

    return math.sqrt(variance(x))

num_friends = [random.randint(50, 100) for _ in range(20)]

print(num_friends)

print(variance(num_friends))

print(standard_deviation(num_friends))

 

INTERQUARTILE RANGE

def quantile(x, p):

    p_index = int(p * len(x))

    return sorted(x)[p_index]

def interquartile_range(x):

    return quantile(x, 0.75) - quantile(x, 0.25)

print(interquartile_range(num_friends))

iv.) Assume

·        You work with DataSciencester, an imaginary social media site.

·        The VP of Growth suspects:

"The more friends someone has on the site, the more time they spend using it."

·        What she means is :
People with more friends are likely to spend more minutes per day on the platform.

Test this theory with data. Use Correlation. Demonstrate the same with Heatmap.

CODE :

import random

import math

import numpy as np

import seaborn as sns

import matplotlib.pyplot as plt

def mean(x):

    return sum(x) / len(x)

def de_mean(x):

    x_bar = mean(x)

    return [x_i - x_bar for x_i in x]

def dot(v, w):

    return sum(v_i * w_i for v_i, w_i in zip(v, w))

def covariance(x, y):

    n = len(x)

    return dot(de_mean(x), de_mean(y)) / (n - 1)

def standard_deviation(x):

    n = len(x)

    deviations = de_mean(x)

    variance = dot(deviations, deviations) / (n - 1)

    return math.sqrt(variance)

def correlation(x, y):

    stdev_x = standard_deviation(x)

    stdev_y = standard_deviation(y)

    return covariance(x, y) / (stdev_x * stdev_y)

num_friends = [random.randint(50, 100) for _ in range(20)]

daily_minutes = [random.randint(50, 100) for _ in range(20)]

print("Number of friends:", num_friends)

print("Daily minutes spent:", daily_minutes)

corr_value = correlation(num_friends, daily_minutes)

print("Correlation coefficient:", corr_value)

corr_matrix = np.array([

    [1.0, corr_value],

    [corr_value, 1.0]

])

sns.heatmap(corr_matrix, annot=True, fmt=".2f",

            xticklabels=["Num Friends", "Daily Minutes"],

            yticklabels=["Num Friends", "Daily Minutes"],

            cmap='coolwarm')

plt.title("Correlation Matrix Heatmap")

plt.show()

 


 

PRACTICAL 4

PROBABILITY

i.)Use Conditional Probability, A die is rolled. Find the probability that the outcome is even, given that it is greater than 2.

CODE :

sample_space = [1, 2, 3, 4, 5, 6]

event_A = [2, 4, 6]           

event_B = [3, 4, 5, 6]         

A_and_B = list(set(event_A).intersection(event_B))

P_B = len(event_B) / len(sample_space)    

P_A_and_B = len(A_and_B) / len(sample_space)  

P_A_given_B = P_A_and_B / P_B

print(f"P(A | B) = {P_A_given_B:.2f}")

 

ii.)Use Conditional Probability, Two fair six-sided dice are rolled.
What is the probability that the sum is 9, given that the sum is odd?

CODE :

sample_space = [(i, j) for i in range(1, 7) for j in range(1, 7)]

event_A = [(i, j) for (i, j) in sample_space if i + j == 9]

event_B = [(i, j) for (i, j) in sample_space if (i + j) % 2 != 0]

A_and_B = [pair for pair in event_A if pair in event_B]

P_B = len(event_B) / len(sample_space)

P_A_and_B = len(A_and_B) / len(sample_space)

P_A_given_B = P_A_and_B / P_B

print(f"Total outcomes: {len(sample_space)}")

print(f"Event A (sum = 9): {event_A}")

print(f"Event B (sum is odd): {len(event_B)} outcomes")

print(f"A ∩ B: {A_and_B}")

print(f"P(A | B) = {P_A_given_B:.3f}")

 

iii.)Use Bayes’ Theorem - Medical Test

1% of people have a rare disease.
The test is 99% accurate:

·        If you have the disease, the test is positive 99% of the time.

·        If you don’t have it, the test is falsely positive 1% of the time.

If a person tests positive, what is the probability they actually have the disease?

CODE :

P_disease = 0.01                 

P_no_disease = 1 - P_disease      

P_pos_given_disease = 0.99  

P_pos_given_no_disease = 0.01   

P_positive = (P_pos_given_disease * P_disease) + (P_pos_given_no_disease * P_no_disease)

P_disease_given_positive = (P_pos_given_disease * P_disease) / P_positive

print(f"Probability of having disease given a positive test: {P_disease_given_positive:.3f}")

 

PRACTICAL : 5

Ø  IMPORTING MODULES

import pandas as pd

import seaborn as sns

import matplotlib.pyplot as plt

import numpy as np

from sklearn.preprocessing import LabelEncoder

from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split

from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import accuracy_score

from sklearn.metrics import confusion_matrix

 

Ø  IMPORTING CSV FILE DATA

df_net=pd.read_csv("C:/Users/IT-LAB/Documents/idsdata.csv")

 

Ø  CHECKING DATA

df_net.drop(columns= ['User ID'], inplace=True)

df_net.head()

 

Ø  DELETING OR DROPPING THE COLUMN NAME “USER ID”

df_net.describe()

 

Ø  PLOTTING IT AS HISTOGRAM

sns.displot(df_net['EstimatedSalary'])

 

Ø  TRANSFORMING THE VALUE OF GENDER INTO NUMERICAL

le = LabelEncoder()

df_net['Gender'] = le.fit_transform(df_net['Gender'])

df_net.head()

 

Ø  CHECKING THE CORRELATION

df_net.corr()


sns.heatmap(df_net.corr())

 

 

Ø  GENDER COLUMN IS NOT NEEDED SO DROPPING IT

df_net.drop(columns= ['Gender'], inplace=True)

df_net.head()

 

 

 

sns.heatmap(df_net.corr()) #gender column removed

 

 

X= df_net.iloc[:,:-1].values

Y = df_net.iloc[:,-1].values

 

X

 

 

 

Y

 

Ø  SPLITTING DATA

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,random_state=True)

 

X_test

 

 

Y_test

 

 

 

 

 

 

sc=StandardScaler()

X_train=sc.fit_transform(X_train)

X_test=sc.transform(X_test)

 

X_train

 

 

X_test

 

 

classifier=GaussianNB()

classifier.fit(X_train,Y_train)

 


 

 

 

 

 

 

Y_pred=classifier.predict(X_test)

print(np.concatenate((Y_pred.reshape(len(Y_pred),1),Y_test.reshape(len(Y_test),1)),1))

 

Ø  ACCURACY TEST

accuracy_score(Y_test,Y_pred)


Ø  HEATMAP

cf_matrix=confusion_matrix(Y_test,Y_pred)

sns.heatmap(cf_matrix, annot=True,fmt='d',cmap='Blues',cbar=True)

 

print(classifier.predict(sc.transform([[45,97000]])))


 

PRACTICAL NO : 6

Create a classification model using the Support Vector Machine algorithm on the dataset ‘Social_Network_Ads.csv’  to predict whether a customer will purchase an insurance policy or not.

CODE :

1. IMPORT THE LIBRARIES

import numpy as np

import matplotib.pyplot as plt

import pandas as pd

2.LOAD THE DATASET-ADDING THE PATH

dataset = pd.read_csv(r'C:\Users\IT-LAB\Documents\Social_Network_Ads.csv')

3.SPLIT DATASET INTO X AND Y

X=dataset.iloc[:,[2,3]].values

Y=dataset.iloc[:,4].values

 

 

 

4.SPLIT THE X AND Y DATASET INTO TRANING AND TEST SET

from sklearn.model_selection import train_test_split

X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.25,random_state=0)

 

5.PERFORM FEATURE SCALING

from sklearn.preprocessing import StandardScaler

sc=StandardScaler()

X_train=sc.fit_transform(X_train)

X_test=sc.transform(X_test)

 

6.FIT SVM IN THE TRAINING SET

from sklearn.svm import SVC

classifier=SVC(kernel='rbf',random_state=0)

classifier.fit(X_train,Y_train)

 

7.PREDCIT THE TEST SET RESULTS

Y_pred=classifier.predict(X_test)

 

 

ap = pd.DataFrame({'Actual values' : Y_test,'Predicted values' : Y_pred})

ap

 

 

9.MAKE THE CONFUSION MATRIX

from sklearn.metrics import confusion_matrix,accuracy_score

cm=confusion_matrix(Y_test,Y_pred)

print(cm)

accuracy_score(Y_test,Y_pred)

 

Practical no 7: K Means Clustering

# Importing Libraries

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler

 

# Load the customer data into a DataFrame

customer_df = pd.read_csv('Mall_Customers.csv')

 

# Check the first 5 rows

print(customer_df.head())

 

# Exploring the data

print(customer_df.info())

print(customer_df.describe())

 

# Check for null values

print(customer_df.isnull().sum())

 

# Get the relevant columns for clustering

X = customer_df.iloc[:, [3, 4]].values   # Using Annual Income and Spending Score columns

 

# Data Transformation

scaler = StandardScaler()

X_scaled = scaler.fit_transform(X)

 

# Finding the optimal number of clusters using the Elbow Method

wcss = []

for i in range(1, 13):

    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)

    kmeans.fit(X_scaled)

    wcss.append(kmeans.inertia_)

 

# Plot the Elbow Graph

plt.figure(figsize=(8, 5))

plt.plot(range(1, 13), wcss, marker='o', linestyle='--')

plt.title('Elbow Method')

plt.xlabel('Number of Clusters')

plt.ylabel('WCSS')

plt.show()

 

# Create the final KMeans model

kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)

y_kmeans = kmeans.fit_predict(X_scaled)

 

# Visualize the clusters

plt.figure(figsize=(8, 6))

plt.scatter(X_scaled[y_kmeans == 0, 0], X_scaled[y_kmeans == 0, 1], s=50, c='red', label='Cluster 1')

plt.scatter(X_scaled[y_kmeans == 1, 0], X_scaled[y_kmeans == 1, 1], s=50, c='blue', label='Cluster 2')

plt.scatter(X_scaled[y_kmeans == 2, 0], X_scaled[y_kmeans == 2, 1], s=50, c='green', label='Cluster 3')

plt.scatter(X_scaled[y_kmeans == 3, 0], X_scaled[y_kmeans == 3, 1], s=50, c='cyan', label='Cluster 4')

plt.scatter(X_scaled[y_kmeans == 4, 0], X_scaled[y_kmeans == 4, 1], s=50, c='magenta', label='Cluster 5')

plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='yellow', marker='*', label='Centroids')

plt.title('Customer Segments using K-Means Clustering')

plt.xlabel('Annual Income (scaled)')

plt.ylabel('Spending Score (scaled)')

plt.legend()

plt.show()

 

# Predict cluster for a new customer

new_customer = np.array([[70, 40]])   # Example: Annual Income = 70, Spending Score = 40

new_customer_scaled = scaler.transform(new_customer)

predicted_cluster = kmeans.predict(new_customer_scaled)

print("Predicted Cluster for New Customer:", predicted_cluster)

PRACTICAL NO : 8

SIMPLE LINEAR REGRESSION

v  Import the libraries

import pandas as pd

import numpy as np

import matplotlib.pyplot as plt

import seaborn as sns

from sklearn.model_selection import train_test_split

from pandas.core.common import random_state

from sklearn.linear_model import LinearRegression

 

v  Load the dataset

df_sal = pd.read_csv(r'C:\Users\IT-LAB\Downloads\Salary_Data.csv')

df_sal.head()

 

v  Data analysis

df_sal.describe()

 

v  Data distribution

plt.title('Salary Distribution Plot')

sns.distplot(df_sal['Salary'])

plt.show()

 

v  Relationship between salary and experience

plt.scatter(df_sal['YearsExperience'], df_sal['Salary'], color = 'green')

plt.title('Salary vs Experience')

plt.xlabel('Years of Experience')

plt.ylabel('Salary')

plt.box(False)

plt.show()

 

v  Split the data set into dependent/independent variables

X = df_sal.iloc[:, :1]  # independent

y = df_sal.iloc[:, 1:]  # dependent

7.)Split data into Train/Test sets

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

 

v  Train the regression model

regressor = LinearRegression()

regressor.fit(X_train, y_train)

 

v  Predict the result

y_pred_train = regressor.predict(X_train) # predicted value of y_train

y_pred_test = regressor.predict(X_test)   # predicted value of y_test

 

v  Plot the train and test result

plt.scatter(X_train, y_train, color = 'lightcoral')

plt.plot(X_train, y_pred_train, color = 'green')

#creates a line plot using the plot function

plt.title('Salary vs Experience (Training Set)')

plt.xlabel('Years of Experience')

plt.ylabel('Salary')

plt.legend(['X_train/ y_pred_train ', 'X_train/y_train'], title = 'Sal/Exp', loc='best', facecolor='white')

plt.box(False)

plt.show()

 

v  Plot test data vs predictions

plt.scatter(X_test, y_test, color = 'green')

plt.plot(X_test, y_pred_test, color = 'blue')

plt.title('Salary vs Experience (Test Set)')

plt.xlabel('Years of Experience')

plt.ylabel('Salary')

plt.legend(['X_test/ y_pred_test', 'X_test/y_test'], title = 'Sal/Exp', loc='best', facecolor='white')

plt.box(False)

plt.show()

PRACTICAL NO : 9

MULTIPLE LINEAR REGRESSION

·        Importing the Libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

·        Reading the Dataset

dataset = pd.read_csv("advertising.csv")

dataset.head()

 

·        Setting the values for independent (X) variable and dependent (Y) variable

x = dataset[['TV', 'Radio', 'Newspaper']]

y = dataset['Sale']

·        Splitting the dataset into train and test set

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 100)

·        Implementing the linear model

from sklearn.linear_model import LinearRegression

mlr = LinearRegression() 

mlr.fit(x_train, y_train)

 

·        Model equation

print("Intercept: ", mlr.intercept_)

print("Coefficients:")

list(zip(x, mlr.coef_))

 

·        Prediction on the test set

y_pred_mlr= mlr.predict(x_test)

print("Prediction for test set: {}".format(y_pred_mlr))

 

·        Actual values and predicted values

mlr_diff = pd.DataFrame({'Actual value': y_test, 'Predicted value': y_pred_mlr})

mlr_diff.head()

 

·        Evaluating the model

print('R squared: {:.2f}'.format(mlr.score(x,y)*100))


Note

mlr.score(x, y)

 

·        Prediction for new set of data using the trained model

new_data = pd.DataFrame({

    'TV': [150, 50],

    'Radio': [30, 20],

    'Newspaper': [40, 10]

})

new_predictions = mlr.predict(new_data)

print("Predictions for new data:", new_predictions)

 

 

 

 

 

 

 

 

 

 


 

PRACTICAL NO : 10

LOGISTIC REGRESSION

Loading Data
import pandas as pd

col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']

pima = pd.read_csv(r"C:\Users\IT-LAB\Documents\diabetes.csv", header=None, names=col_names)

pima.head()

 

Selecting Feature
feature_cols = ['pregnant', 'insulin', 'bmi', 'age','glucose','bp','pedigree']

X = pima[feature_cols] # Features

y = pima.label # Target variable

Splitting Data
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)

Model Development and Prediction
numeric_columns = ['pregnant', 'insulin', 'bmi', 'age', 'glucose', 'bp', 'pedigree']

X_train[numeric_columns] = X_train[numeric_columns].apply(pd.to_numeric, errors='coerce')

X_train.fillna(X_train.mean(), inplace=True)

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()

logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)

numeric_columns = ['pregnant', 'insulin', 'bmi', 'age', 'glucose', 'bp', 'pedigree']

X_train[numeric_columns] = X_train[numeric_columns].apply(pd.to_numeric, errors='coerce')

X_train.fillna(X_train.mean(), inplace=True)

from sklearn.linear_model import LogisticRegression

logreg = LogisticRegression()

logreg.fit(X_train, y_train)

y_pred = logreg.predict(X_test)

 

Actual values and the predicted values

logreg_diff = pd.DataFrame({'Actual value': y_test, 'Predicted value': y_pred})

logreg_diff.head()

 

Model Evaluation using Confusion Matrix
from sklearn import metrics

cnf_matrix = metrics.confusion_matrix(y_test, y_pred)

cnf_matrix


Visualizing Confusion Matrix using Heatmap
import seaborn as sns

import matplotlib.pyplot as plt

sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap="YlGnBu" ,fmt='g')

plt.title('Confusion matrix')

plt.ylabel('Actual label')

plt.xlabel('Predicted label')

 

Test for accuracy

from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_test, y_pred)

print("Test Accuracy: {:.2f}%".format(accuracy * 100))


Prediction for a new data using the Model

new_data = pd.DataFrame({

    'pregnant': [2],

    'insulin': [80],

    'bmi': [25.6],

    'age': [35],

    'glucose': [120],

    'bp': [70],

    'pedigree': [0.5]

})

new_data[numeric_columns] = new_data[numeric_columns].apply(pd.to_numeric, errors='coerce')

new_data.fillna(X_train.mean(), inplace=True)

new_pred_class = logreg.predict(new_data)

print("Predicted class:", new_pred_class)

new_pred_proba = logreg.predict_proba(new_data)

print("Probability of class 1 (diabetes):", new_pred_proba[0][1])

OUTPUT :

 

 


 


PRACTICAL NO : 11

Design DBSCAN Clustering Model using the Data Set ‘Mall_Customers.csv’ to group the data into respective numbers of clusters. Determine the number of clusters using the Elbow/knee Method, also predict the cluster numbers for a new sets of data.

 

1.      Load Dataset Set

import pandas as pd

from sklearn.cluster import DBSCAN

from sklearn.preprocessing import StandardScaler

# Load dataset

df = pd.read_csv(r"C:\Users\IT-LAB\Downloads\Mall_Customers.csv")

print(df.head())

 

2. Data Preprocessing

X = df[['Annual Income (k$)', 'Spending Score (1-100)']]

X

 

 

Step 2: Scaling

scaler = StandardScaler()

X_scaled = scaler.fit_transform(X)

X_scaled

 

DBSCAN Clustering

from sklearn.neighbors import NearestNeighbors

import matplotlib.pyplot as plt

import numpy as np

 

# Find optimal eps

neighbors = NearestNeighbors(n_neighbors=4)

neighbors_fit = neighbors.fit(X_scaled)

distances, indices = neighbors_fit.kneighbors(X_scaled)

distances = np.sort(distances[:, -1], axis=0)

 

plt.plot(distances)

plt.xlabel('Points')

plt.ylabel('Epsilon')

plt.title('k-Distance Graph for eps Selection')

 

Fit DBSCAN

dbscan = DBSCAN(eps=0.3, min_samples=5)

clusters = dbscan.fit_predict(X_scaled)

 

# Add cluster labels to original data

df['Cluster'] = clusters

print(df['Cluster'].value_counts())


Visualization

plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap='viridis', s=50)

plt.xlabel('Annual Income (scaled)')

plt.ylabel('Spending Score (scaled)')

plt.title('DBSCAN Clustering (eps=0.3, min_samples=5)')

plt.colorbar(label='Cluster ID')

 

Predicting Clusters for New Data

Step 1: Preprocess New Data

new_data = pd.DataFrame({

    'Annual Income (k$)': [15, 120, 45, 80, 30], 

    'Spending Score (1-100)': [96, 10, 60, 15, 85]

})

# Scale using the same scaler

new_scaled = scaler.fit_transform(new_data)

 

Step 2: Predict Cluster

# Use DBSCAN's `fit_predict` to assign clusters

new_clusters = dbscan.fit_predict(np.vstack([X_scaled, new_scaled]))

 

# Extract predictions for new data only

new_data['Cluster'] = new_clusters[-len(new_data):]

print(new_data)

 

 

 

 

 

This paste expires in <1 day. Public IP access. Share whatever you see with others in seconds with Context.Terms of ServiceReport this